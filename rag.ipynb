{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dntfreitas/introduction-agents-ai/blob/main/rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2e30f966d999f646"
      },
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG) with OpenAI API\n",
        "\n",
        "In this notebook, we'll explore the concept of Retrieval-Augmented Generation (RAG) and how to use it with OpenAI's API. This approach combines the strengths of information retrieval and text generation to create more informed and accurate responses.\n",
        "\n",
        "### What is RAG?\n",
        "RAG enhances the response generation process by incorporating relevant external documents or data. It involves two main steps:\n",
        "1. **Retrieve**: Fetch relevant documents from a knowledge base.\n",
        "2. **Generate**: Use a language model (e.g., OpenAI GPT) to generate an answer based on both the query and retrieved documents.\n",
        "\n",
        "We'll walk through a basic example using PDF documents stored in a local directory."
      ],
      "id": "2e30f966d999f646"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:05:26.485562Z",
          "start_time": "2025-05-19T20:05:21.021791Z"
        },
        "id": "f344c28092df14e5"
      },
      "cell_type": "code",
      "source": [
        "# Let's make sure we have the required libraries installed for this tutorial.\n",
        "!pip install openai faiss-cpu tiktoken PyPDF2 langchain gradio"
      ],
      "id": "f344c28092df14e5",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:05:31.326515Z",
          "start_time": "2025-05-19T20:05:31.295828Z"
        },
        "id": "b4dc47bad0f108b8"
      },
      "cell_type": "code",
      "source": [
        "# Now, let's import the necessary libraries and set up our environment.\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import PyPDF2\n",
        "import faiss\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from IPython.display import Markdown, display\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from openai import OpenAI"
      ],
      "id": "b4dc47bad0f108b8",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e253fa62347dbb45"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# As we are going to use Google Coolab, we don't need to load the environment variables.\n",
        "# Otherwise, you can use the following code to load the environment variables from a `.env` file.\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv(override=True)\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "id": "e253fa62347dbb45"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T19:18:08.034723Z",
          "start_time": "2025-05-19T19:18:07.993678Z"
        },
        "id": "e6baa4d4f6f2c62c"
      },
      "cell_type": "code",
      "source": [
        "# Now, let's initialize the OpenAI API client.\n",
        "openai = OpenAI(api_key = OPENAI_API_KEY)"
      ],
      "id": "e6baa4d4f6f2c62c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:08:35.772785Z",
          "start_time": "2025-05-19T20:08:29.027811Z"
        },
        "id": "88b812cb62c1f9ec"
      },
      "cell_type": "code",
      "source": [
        "# Load and Extract Text from PDFs (code)\n",
        "def load_pdfs_from_directory(directory_path: str) -> List[str]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            filepath = os.path.join(directory_path, filename)\n",
        "            with open(filepath, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "                # Chunk the text\n",
        "                chunks = text_splitter.split_text(text)\n",
        "                documents.extend(chunks)\n",
        "    return documents\n",
        "\n",
        "\n",
        "pdf_directory = \"./pdfs\"\n",
        "documents = load_pdfs_from_directory(pdf_directory)"
      ],
      "id": "88b812cb62c1f9ec",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d9d99a92e940f4ea"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n",
        "Embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n",
        "- Search (where results are ranked by relevance to a query string)\n",
        "- Clustering (where text strings are grouped by similarity)\n",
        "- Recommendations (where items with related text strings are recommended)\n",
        "- Anomaly detection (where outliers with little relatedness are identified)\n",
        "- Diversity measurement (where similarity distributions are analyzed)\n",
        "- Classification (where text strings are classified by their most similar label)\n",
        "\n",
        "**An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.**\n",
        "\n",
        "Source: [OpenAI](https://platform.openai.com/docs/guides/embeddings).\n"
      ],
      "id": "d9d99a92e940f4ea"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:11:04.359372Z",
          "start_time": "2025-05-19T20:11:04.339265Z"
        },
        "id": "c19ddf1b46b04783"
      },
      "cell_type": "code",
      "source": [
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    response = openai.embeddings.create(\n",
        "        input=texts,\n",
        "        model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    return [e.embedding for e in response.data]"
      ],
      "id": "c19ddf1b46b04783",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:11:10.879988Z",
          "start_time": "2025-05-19T20:11:05.731565Z"
        },
        "id": "a5ed0581509906da"
      },
      "cell_type": "code",
      "source": [
        "embeddings = embed_texts(documents)"
      ],
      "id": "a5ed0581509906da",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eb2d241b080c542c"
      },
      "cell_type": "markdown",
      "source": [
        "# Build FAISS Index\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.\n",
        "For the RAG pipeline, we will use FAISS to index the embeddings of the documents. This allows us to quickly retrieve the most relevant documents based on a query."
      ],
      "id": "eb2d241b080c542c"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:11:24.710579Z",
          "start_time": "2025-05-19T20:11:24.581470Z"
        },
        "id": "fb3b23f85e32d038"
      },
      "cell_type": "code",
      "source": [
        "# Build FAISS Index\n",
        "dim = len(embeddings[0])  # dimension of the embeddings\n",
        "index = faiss.IndexFlatL2(dim)  # L2 distance (Euclidean distance)\n",
        "index.add(np.array(embeddings).astype('float32'))  # add embeddings to the index"
      ],
      "id": "fb3b23f85e32d038",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "de4857a4b6d3a3c9"
      },
      "cell_type": "markdown",
      "source": [
        "# Retrieve Relevant Documents\n",
        "\n",
        "The retrieve function takes a query string and retrieves the top k most relevant documents from the FAISS index. It does this by embedding the query and searching for the nearest neighbors in the index."
      ],
      "id": "de4857a4b6d3a3c9"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:11:46.958914Z",
          "start_time": "2025-05-19T20:11:46.946571Z"
        },
        "id": "b801e5103ec5ee76"
      },
      "cell_type": "code",
      "source": [
        "# Retrieve Relevant Documents\n",
        "def retrieve(query: str, k: int = 50) -> List[str]:\n",
        "    query_embedding = embed_texts([query])[0]\n",
        "    D, I = index.search(np.array([query_embedding]).astype('float32'), k)\n",
        "    # D contains distances, I contains indices of the nearest neighbors\n",
        "    return [documents[i] for i in I[0]]"
      ],
      "id": "b801e5103ec5ee76",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:11:48.305192Z",
          "start_time": "2025-05-19T20:11:47.970069Z"
        },
        "id": "38d881ef7a55ecf1"
      },
      "cell_type": "code",
      "source": [
        "retrieved = retrieve(\"Talk about Lisbon\")"
      ],
      "id": "38d881ef7a55ecf1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:20:51.999257Z",
          "start_time": "2025-05-19T20:20:51.993186Z"
        },
        "id": "4af552df8a768e34"
      },
      "cell_type": "code",
      "source": [
        "def generate_answer(query: str, retrieved_docs: List[str]) -> str:\n",
        "    context = \"\\n\".join(retrieved_docs)\n",
        "    prompt = f\"\"\"\n",
        "    Answer the question based only the context below.\n",
        "    If the context does not contain the answer, say \"I don't know\".\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "id": "4af552df8a768e34",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:20:58.366587Z",
          "start_time": "2025-05-19T20:20:54.194242Z"
        },
        "id": "4ee22a1b281474ec"
      },
      "cell_type": "code",
      "source": [
        "question = \"What are the main attractions in Lisbon?\"\n",
        "most_relevant_documents = retrieve(question)\n",
        "completion = generate_answer(question, documents)"
      ],
      "id": "4ee22a1b281474ec",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:20:58.398614Z",
          "start_time": "2025-05-19T20:20:58.390109Z"
        },
        "id": "e8ec0c3bfdcb0938"
      },
      "cell_type": "code",
      "source": [
        "display(Markdown(completion))"
      ],
      "id": "e8ec0c3bfdcb0938",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:21:00.639186Z",
          "start_time": "2025-05-19T20:20:58.413982Z"
        },
        "id": "6d58972e1ed9df11"
      },
      "cell_type": "code",
      "source": [
        "question = \"What are the main attractions in Jupyter?\"\n",
        "most_relevant_documents = retrieve(question)\n",
        "completion = generate_answer(question, documents)"
      ],
      "id": "6d58972e1ed9df11",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:21:00.664259Z",
          "start_time": "2025-05-19T20:21:00.661499Z"
        },
        "id": "d1f8e5e5ff38f353"
      },
      "cell_type": "code",
      "source": [
        "display(Markdown(completion))"
      ],
      "id": "d1f8e5e5ff38f353",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3f18cfdb46a6e4c0"
      },
      "cell_type": "markdown",
      "source": [
        "# Build a RAG Pipeline\n",
        "\n",
        "The RAG pipeline combines the retrieval and generation steps into a single function. It takes a query string, retrieves relevant documents, and generates an answer based on those documents."
      ],
      "id": "3f18cfdb46a6e4c0"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:21:00.698560Z",
          "start_time": "2025-05-19T20:21:00.695903Z"
        },
        "id": "bf1e51c648936d40"
      },
      "cell_type": "code",
      "source": [
        "def rag_pipeline(query: str) -> str:\n",
        "    retrieved_docs = retrieve(query)\n",
        "    answer = generate_answer(query, retrieved_docs)\n",
        "    return answer"
      ],
      "id": "bf1e51c648936d40",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:21:07.692134Z",
          "start_time": "2025-05-19T20:21:00.713212Z"
        },
        "id": "6b4bc8039eadd7c9"
      },
      "cell_type": "code",
      "source": [
        "display(Markdown(rag_pipeline(\"Talk about Lisbon\")))"
      ],
      "id": "6b4bc8039eadd7c9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e9e90a09091f9562"
      },
      "cell_type": "markdown",
      "source": [
        "# Build a Q&A Interface using Gradio\n",
        "\n",
        "Gradio is a Python library that allows you to quickly create user interfaces for machine learning models. We'll use Gradio to build a simple chat interface for our RAG pipeline."
      ],
      "id": "e9e90a09091f9562"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-19T20:23:03.136875Z",
          "start_time": "2025-05-19T20:23:02.873488Z"
        },
        "id": "9c994d7eca0e9384"
      },
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    fn=rag_pipeline,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"RAG with OpenAI and PDF Knowledge Base\",\n",
        "    description=\"Ask questions based on content extracted from your local PDF files.\"\n",
        ").launch()"
      ],
      "id": "9c994d7eca0e9384",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}