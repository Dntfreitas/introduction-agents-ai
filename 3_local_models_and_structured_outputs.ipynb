{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dntfreitas/introduction-agents-ai/blob/main/3_local_models_and_structured_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fa54c4b613af7145"
      },
      "cell_type": "markdown",
      "source": [
        "**Introduction to Ollama**\n",
        "\n",
        "Ollama is a powerful command-line tool that makes it easy to run and interact with large language models locally. Designed for speed, privacy, and simplicity, Ollama enables developers and enthusiasts to use open models directly on their machines—no cloud required.\n",
        "\n",
        "**Pulling a Model: `ollama pull`**\n",
        "\n",
        "Before running a model, you need to download it using the `pull` command:\n",
        "\n",
        "```\n",
        "ollama pull <model>\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "ollama pull llama3\n",
        "```\n",
        "\n",
        "This downloads the `llama3` model to your local system so it’s ready for use.\n",
        "\n",
        "**Running a Model: `ollama run`**\n",
        "\n",
        "Once the model is pulled, start an interactive session with:\n",
        "\n",
        "```\n",
        "ollama run <model>\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "ollama run llama3\n",
        "```\n",
        "\n",
        "This opens a terminal chat session where you can interact with the model in real time.\n",
        "\n",
        "## OpenAI-Compatible API\n",
        "\n",
        "Ollama also provides an OpenAI-compatible API interface. This means you can use local models in place of OpenAI's models in existing applications with minimal changes—ideal for developers who want to integrate open-source LLMs into their tools while maintaining control and privacy.\n",
        "\n",
        "\n",
        "Please, keep into account that the models are large and may take a while to download. Also, check your hardware requirements to ensure you have enough resources to run the models locally.\n",
        "\n",
        "> [!CAUTION]\n",
        "> This notebook must be run in your local environment, not in Google Colab!\n",
        "\n",
        "# Local Models: Advantages and Disadvantages\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- **Privacy**: Your data stays on your machine, reducing the risk of data leaks.\n",
        "- **Cost**: No ongoing cloud costs; you only pay for the hardware.\n",
        "- **Customization**: You can fine-tune models to better suit your specific needs.\n",
        "- **No API Limits**: You are not subject to API rate limits or usage caps.\n",
        "- **Offline Access**: You can run models without an internet connection.\n",
        "\n",
        "## Disadvantages\n",
        "- **Hardware Requirements**: Running large models requires significant computational resources (GPU/TPU).\n",
        "- **Setup Complexity**: Initial setup can be more complex than using a cloud API.\n",
        "- **Maintenance**: You are responsible for maintaining and updating the models."
      ],
      "id": "fa54c4b613af7145"
    },
    {
      "metadata": {
        "id": "1e8c2e01f50ac7d4"
      },
      "cell_type": "code",
      "source": [
        "# Start by pulling the model\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull deepseek-r1:7b"
      ],
      "id": "1e8c2e01f50ac7d4",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a4174575bc67d6fd"
      },
      "cell_type": "code",
      "source": [
        "# Now, jump into the interactive shell and run the model\n",
        "!ollama run gemma3:1b"
      ],
      "id": "a4174575bc67d6fd",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7f92d5beb241f708"
      },
      "cell_type": "code",
      "source": [
        "# Congratulations! You have successfully pulled and run a model using Ollama."
      ],
      "id": "7f92d5beb241f708",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a72c104751c38309"
      },
      "cell_type": "code",
      "source": [
        "# Now, let's import the necessary libraries and set up our environment.\n",
        "\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n"
      ],
      "id": "a72c104751c38309",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cbea900c8ffc8754"
      },
      "cell_type": "code",
      "source": [
        "# Load the environment variables from a `.env` file\n",
        "\n",
        "load_dotenv(override=True)"
      ],
      "id": "cbea900c8ffc8754",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "24f3220984a033ff"
      },
      "cell_type": "code",
      "source": [
        "# Now, let's initialize some models\n",
        "\n",
        "gemma = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"gemma\")\n",
        "deepseek = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"deepseek\")\n",
        "gpt = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "models = {\n",
        "    \"gemma3:1b\": gemma,\n",
        "    \"deepseek-r1:7b\": deepseek,\n",
        "    \"gpt-4.1-nano\": gpt,\n",
        "}"
      ],
      "id": "24f3220984a033ff",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6868c549e066b96b"
      },
      "cell_type": "markdown",
      "source": [
        "# Structured Outputs\n",
        "\n",
        "Structured outputs are a powerful feature of the OpenAI API that allows you to define the format of the output you want from the model. This is particularly useful when you need the model to return data in a specific structure, such as JSON or a table."
      ],
      "id": "6868c549e066b96b"
    },
    {
      "metadata": {
        "id": "6aedd8903feaccde"
      },
      "cell_type": "code",
      "source": [
        "# We are going PyDantic to define structured data types for our models\n",
        "class Competitor(BaseModel):\n",
        "    \"\"\"\n",
        "    Competitor model to store the name and score of the competitor.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    \"\"\" The name of the competitor. \"\"\"\n",
        "    score: float\n",
        "    \"\"\" The score of the competitor. The score is a integer between 0 and 20, with higher scores being better. \"\"\"\n",
        "    reason: str\n",
        "    \"\"\" The reason for the score. \"\"\""
      ],
      "id": "6aedd8903feaccde",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b80afb252bf188c5"
      },
      "cell_type": "code",
      "source": [
        "prompt = \"Write a touristic guide for the city of Funchal, Madeira.\""
      ],
      "id": "b80afb252bf188c5",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4aa76132e17f258d"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_response(prompt, model_name, model):\n",
        "    \"\"\"\n",
        "    Evaluate the response of a model to a given prompt.\n",
        "    \"\"\"\n",
        "    # Call the model to get the response\n",
        "    response_model = model.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    response = response_model.choices[0].message.content\n",
        "\n",
        "    # Evaluate the response\n",
        "    evaluation = gpt.beta.chat.completions.parse(\n",
        "        model=\"gpt-4.1-2025-04-14\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Evaluate the following response for clarity and strength of argument: {response}\"},\n",
        "        ],\n",
        "        response_format=Competitor,\n",
        "    )\n",
        "\n",
        "    return evaluation, response"
      ],
      "id": "4aa76132e17f258d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cb1356be7b533f14"
      },
      "cell_type": "code",
      "source": [
        "# Iterate over the models and get their evaluations\n",
        "evaluations = []\n",
        "for name, model in models.items():\n",
        "    competitor, response = evaluate_response(prompt, name, model)\n",
        "    evaluations.append(\n",
        "        {\n",
        "            \"name\": name,\n",
        "            \"evaluation\": competitor.choices[0].message.parsed,\n",
        "            \"response\": response,\n",
        "        }\n",
        "    )"
      ],
      "id": "cb1356be7b533f14",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "51926eee62a33e33"
      },
      "cell_type": "code",
      "source": [
        "# Select the best response\n",
        "best_response_idx = max(\n",
        "    range(len(evaluations)),\n",
        "    key=lambda i: evaluations[i][\"evaluation\"].score,\n",
        ")"
      ],
      "id": "51926eee62a33e33",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64bc673aa63d8866"
      },
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "\n",
        "The best response was given by the model:{evaluations[best_response_idx][\"name\"]}\n",
        "The score was: {evaluations[best_response_idx][\"evaluation\"].score}\n",
        "The reason for the score was: {evaluations[best_response_idx][\"evaluation\"].reason}\n",
        "The answer was: {evaluations[best_response_idx][\"response\"]}\n",
        "\n",
        "\"\"\")"
      ],
      "id": "64bc673aa63d8866",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}